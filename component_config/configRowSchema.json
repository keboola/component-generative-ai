{
  "type": "object",
  "title": "Component configuration",
  "required": [
    "model_type",
    "additional_options",
    "prompt_options",
    "destination"
  ],
  "properties": {
    "predefined_model": {
      "type": "string",
      "title": "Model",
      "enum": [
        "text-davinci-003",
        "text-davinci-002",
        "text-curie-001",
        "text-babbage-001",
        "text-ada-001",
        "gpt-4",
        "gpt-4-0314",
        "gpt-4-32k",
        "gpt-4-32k-0314",
        "gpt-3.5-turbo",
        "gpt-3.5-turbo-0301"
      ],
      "description": "The model which will generate the completion. <a href=\"https://beta.openai.com/docs/models\">Learn more.</a>",
      "propertyOrder": 2,
      "options": {
        "dependencies": {
          "model_type": "predefined"
        }
      }
    },
    "custom_model": {
      "type": "string",
      "title": "Model",
      "description": "The model which will generate the completion. <a href=\"https://beta.openai.com/docs/models\">Learn more.</a>",
      "propertyOrder": 2,
      "options": {
        "dependencies": {
          "model_type": "custom"
        }
      }
    },
    "model_type": {
      "type": "string",
      "title": "Model Type",
      "default": "predefined",
      "description": "Pick from predefined or enter a custom model.",
      "enum": [
        "predefined",
        "custom"
      ],
      "propertyOrder": 1
    },
    "additional_options": {
      "type": "object",
      "title": "Model Options",
      "propertyOrder": 10,
      "required": [
        "temperature",
        "top_p",
        "frequency_penalty",
        "presence_penalty"
      ],
      "properties": {
        "max_tokens": {
          "type": "number",
          "title": "Max Tokens",
          "default": 100,
          "description": "The maximum number of tokens to generate in the completion.\n\nThe token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).",
          "propertyOrder": 1
        },
        "temperature": {
          "type": "number",
          "title": "Temperature",
          "default": 0.6,
          "description": "What sampling temperature to use [0-1]. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "propertyOrder": 2
        },
        "top_p": {
          "type": "number",
          "title": "Top P",
          "default": 1,
          "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
          "propertyOrder": 10
        },
        "frequency_penalty": {
          "type": "number",
          "title": "Frequency Penalty",
          "default": 0,
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
          "propertyOrder": 20
        },
        "presence_penalty": {
          "type": "number",
          "title": "Presence Penalty",
          "default": 0,
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
          "propertyOrder": 30
        }
      }
    },
    "prompt_templates": {
      "title": "Prompt Templates",
      "propertyOrder": 15,
      "properties": {
        "prompt_template": {
          "type": "string",
          "title": "Prompt Template",
          "enum": [
            "timestamp_from_date",
            "remove_diacritics",
            "add_diacritics",
            "extract_topics",
            "sentiment_scoring",
            "text_shortener",
            "grammar_correction"
          ],
          "description": "Select the template to load. You can then copy the template into the Prompt window.",
          "propertyOrder": 1,
          "options": {
            "enum_titles": [
              "Timestamp from date",
              "Remove Diacritics",
              "Add diacritics",
              "Extract Topics",
              "Sentiment Scoring",
              "Text Shortener",
              "Grammar Correction"
            ]
          }
        },
        "load_prompt_template": {
          "type": "button",
          "format": "sync-action",
          "propertyOrder": 2,
          "options": {
            "dependencies": {
              "model_type": "predefined"
            },
            "async": {
              "label": "LOAD PROMPT TEMPLATE",
              "action": "getPromptTemplate"
            }
          }
        }
      }
    },
    "prompt_options": {
      "propertyOrder": 20,
      "type": "object",
      "title": "Prompt Options",
      "properties": {
        "prompt": {
          "type": "string",
          "format": "textarea",
          "title": "Prompt",
          "default": "Extract keywords from this text:\n\n\"\"\"\n[[INPUT_COLUMN]]\n\"\"\"",
          "description": "The prompt and data input pattern. Refer to the input column using placeholder [[INPUT_COLUMN]]. The input table must contain the referenced column. You can find best practices for prompt engineering in <a href=\"https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api\">OpenAI help pages</a>.",
          "propertyOrder": 20,
          "options": {
            "input_height": "250px"
          }
        },
        "validation_button": {
          "type": "button",
          "format": "sync-action",
          "propertyOrder": 30,
          "options": {
            "async": {
              "label": "TEST PROMPT",
              "action": "testPrompt"
            }
          }
        }
      }
    },
    "max_token_spend": {
      "title": "Maximum token spend (Optional)",
      "description": "If set to value greater than 0, the component will stop processing rows after reaching the limit of allowed tokens.",
      "default": 0,
      "type": "integer",
      "propertyOrder": 40
    },
    "destination": {
      "title": "Destination",
      "type": "object",
      "required": [
        "output_table_name",
        "incremental_load",
        "primary_keys_array"
      ],
      "properties": {
        "output_table_name": {
          "type": "string",
          "title": "Storage Table Name",
          "description": "Name of the table stored in Storage.",
          "propertyOrder": 100
        },
        "incremental_load": {
          "type": "boolean",
          "format": "checkbox",
          "title": "Incremental Load",
          "description": "If incremental load is turned on, the table will be updated instead of rewritten. Tables with a primary key will have rows updated, tables without a primary key will have rows appended.",
          "propertyOrder": 110
        },
        "primary_keys_array": {
          "type": "array",
          "title": "Primary Keys",
          "format": "select",
          "items": {
            "type": "string"
          },
          "uniqueItems": true,
          "options": {
            "tags": true
          },
          "description": "You can enter multiple columns seperated by commas at once e.g., id, other_id. If a primary key is set, updates can be done on the table by selecting incremental loads. The primary key can consist of multiple columns. The primary key of an existing table cannot be changed.",
          "propertyOrder": 120
        },
        "store_results_on_failure": {
          "type": "boolean",
          "title": "Store Results on Failure",
          "format": "checkbox",
          "description": "If you are running on Queue V2 and set this to true, the component will not quit when it encounters Invalid Request API Error (for example exceeding token limit).",
          "default": false
        }
      }
    }
  }
}